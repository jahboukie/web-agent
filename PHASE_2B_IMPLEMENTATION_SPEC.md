# Phase 2B Implementation Specification: Background Task Processing

**Document Version:** 1.0
**Date:** June 19, 2025
**Target:** Augment Code AI Assistant
**Phase:** Phase 2B - Core Intelligence Implementation
**Architect:** Claude Code

---

## ðŸŽ¯ **IMPLEMENTATION OVERVIEW**

This document provides **detailed implementation specifications** for Augment Code to build the background task processing system for WebAgent. The architecture has been designed by Claude Code and is ready for implementation.

### **What to Build:**
1. **Background Task Infrastructure** - Task status tracking and lifecycle management
2. **WebParser Service** - Async webpage parsing with Playwright
3. **Browser Context Management** - Resource pooling and cleanup
4. **Caching System** - Redis-based webpage caching
5. **Database Enhancements** - Background task processing fields

### **Architecture Foundation:**
- âœ… **Authentication System** - Complete and working
- âœ… **Database Models** - All entities defined and created
- âœ… **FastAPI Structure** - API endpoints and middleware ready
- ðŸŽ¯ **Background Processing** - Ready for implementation

---

## ðŸ“‹ **IMPLEMENTATION PRIORITY ORDER**

### **Priority 1: Task Status Management (Start Here)**
1. Database schema migration for background task fields
2. TaskStatusService for progress tracking
3. Enhanced Task model with background processing capabilities

### **Priority 2: Basic WebParser Service**
1. Browser context management and pooling
2. WebParserService with FastAPI BackgroundTasks
3. Basic webpage parsing with Playwright

### **Priority 3: Caching & Performance**
1. Redis-based webpage caching service
2. Error handling and retry logic
3. Resource cleanup and monitoring

### **Priority 4: Advanced Features**
1. Real-time status updates via WebSocket
2. Enhanced metrics and monitoring
3. Performance optimization

---

## ðŸ› ï¸ **TASK 1: DATABASE SCHEMA MIGRATION**

### **Files to Create/Modify:**
- `alembic/versions/002_background_tasks.py` (create new migration)
- `app/models/task.py` (enhance existing)

### **Database Migration Implementation:**

```python
# alembic/versions/002_background_tasks.py
"""Add background task processing fields

Revision ID: 002_background_tasks
Revises: 001_initial_migration
Create Date: 2025-06-19 10:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '002_background_tasks'
down_revision = '001_initial_migration'
branch_labels = None
depends_on = None

def upgrade():
    # Add background processing fields to tasks table
    op.add_column('tasks', sa.Column('background_task_id', sa.String(255), nullable=True))
    op.add_column('tasks', sa.Column('queue_name', sa.String(100), nullable=False, server_default='default'))
    op.add_column('tasks', sa.Column('worker_id', sa.String(255), nullable=True))
    op.add_column('tasks', sa.Column('processing_started_at', sa.DateTime(timezone=True), nullable=True))
    op.add_column('tasks', sa.Column('processing_completed_at', sa.DateTime(timezone=True), nullable=True))
    op.add_column('tasks', sa.Column('progress_details', sa.JSON(), nullable=False, server_default='{}'))
    op.add_column('tasks', sa.Column('estimated_completion_at', sa.DateTime(timezone=True), nullable=True))
    op.add_column('tasks', sa.Column('memory_usage_mb', sa.Integer(), nullable=True))
    op.add_column('tasks', sa.Column('browser_session_id', sa.String(255), nullable=True))
    op.add_column('tasks', sa.Column('last_error_at', sa.DateTime(timezone=True), nullable=True))
    op.add_column('tasks', sa.Column('error_details', sa.JSON(), nullable=False, server_default='{}'))

    # Add indexes for performance
    op.create_index('idx_tasks_background_task_id', 'tasks', ['background_task_id'])
    op.create_index('idx_tasks_queue_name', 'tasks', ['queue_name'])
    op.create_index('idx_tasks_processing_started_at', 'tasks', ['processing_started_at'])

def downgrade():
    # Remove indexes
    op.drop_index('idx_tasks_background_task_id', table_name='tasks')
    op.drop_index('idx_tasks_queue_name', table_name='tasks')
    op.drop_index('idx_tasks_processing_started_at', table_name='tasks')

    # Remove columns
    op.drop_column('tasks', 'error_details')
    op.drop_column('tasks', 'last_error_at')
    op.drop_column('tasks', 'browser_session_id')
    op.drop_column('tasks', 'memory_usage_mb')
    op.drop_column('tasks', 'estimated_completion_at')
    op.drop_column('tasks', 'progress_details')
    op.drop_column('tasks', 'processing_completed_at')
    op.drop_column('tasks', 'processing_started_at')
    op.drop_column('tasks', 'worker_id')
    op.drop_column('tasks', 'queue_name')
    op.drop_column('tasks', 'background_task_id')
```

### **Enhanced Task Model:**

```python
# Add these fields to existing app/models/task.py Task class
class Task(Base):
    # ... existing fields ...

    # Background processing fields (ADD THESE)
    background_task_id = Column(String(255), nullable=True, index=True)
    queue_name = Column(String(100), default="default", nullable=False)
    worker_id = Column(String(255), nullable=True)
    processing_started_at = Column(DateTime(timezone=True), nullable=True)
    processing_completed_at = Column(DateTime(timezone=True), nullable=True)

    # Progress tracking
    progress_details = Column(JSON, default=dict)  # {"current_step": "extracting_elements", "progress": 45}
    estimated_completion_at = Column(DateTime(timezone=True), nullable=True)

    # Resource tracking
    memory_usage_mb = Column(Integer, nullable=True)
    browser_session_id = Column(String(255), nullable=True)

    # Error handling
    last_error_at = Column(DateTime(timezone=True), nullable=True)
    error_details = Column(JSON, default=dict)
```

---

## ðŸ› ï¸ **TASK 2: TASK STATUS SERVICE**

### **File to Create:**
- `app/services/task_status_service.py`

### **Complete Implementation:**

```python
# app/services/task_status_service.py
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
import structlog
import uuid

from app.models.task import Task, TaskStatus
from app.core.config import settings

logger = structlog.get_logger(__name__)

class TaskStatusService:
    """Service for managing task status and progress tracking."""

    @staticmethod
    async def update_task_progress(
        db: AsyncSession,
        task_id: int,
        status: Optional[TaskStatus] = None,
        progress_percentage: Optional[int] = None,
        current_step: Optional[str] = None,
        estimated_completion: Optional[datetime] = None,
        memory_usage_mb: Optional[int] = None
    ) -> bool:
        """Update task progress with real-time status."""

        try:
            # Build update dictionary
            update_data = {
                "updated_at": datetime.utcnow()
            }

            if status is not None:
                update_data["status"] = status

            if progress_percentage is not None:
                update_data["progress_percentage"] = progress_percentage

            if estimated_completion is not None:
                update_data["estimated_completion_at"] = estimated_completion

            if memory_usage_mb is not None:
                update_data["memory_usage_mb"] = memory_usage_mb

            # Update progress details
            if current_step is not None or progress_percentage is not None:
                # Get existing progress details
                result = await db.execute(select(Task.progress_details).where(Task.id == task_id))
                existing_details = result.scalar_one_or_none() or {}

                if current_step is not None:
                    existing_details["current_step"] = current_step
                if progress_percentage is not None:
                    existing_details["progress"] = progress_percentage
                existing_details["last_updated"] = datetime.utcnow().isoformat()

                update_data["progress_details"] = existing_details

            # Execute update
            await db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(update_data)
            )
            await db.commit()

            logger.info(
                "Task progress updated",
                task_id=task_id,
                status=status,
                progress=progress_percentage,
                step=current_step
            )

            return True

        except Exception as e:
            logger.error("Failed to update task progress", task_id=task_id, error=str(e))
            await db.rollback()
            return False

    @staticmethod
    async def mark_task_processing(
        db: AsyncSession,
        task_id: int,
        worker_id: str,
        browser_session_id: Optional[str] = None
    ) -> bool:
        """Mark task as actively processing."""

        try:
            update_data = {
                "status": TaskStatus.IN_PROGRESS,
                "processing_started_at": datetime.utcnow(),
                "worker_id": worker_id,
                "updated_at": datetime.utcnow()
            }

            if browser_session_id:
                update_data["browser_session_id"] = browser_session_id

            await db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(update_data)
            )
            await db.commit()

            logger.info("Task marked as processing", task_id=task_id, worker_id=worker_id)
            return True

        except Exception as e:
            logger.error("Failed to mark task processing", task_id=task_id, error=str(e))
            await db.rollback()
            return False

    @staticmethod
    async def complete_task(
        db: AsyncSession,
        task_id: int,
        result_data: Dict[str, Any],
        performance_metrics: Optional[Dict[str, Any]] = None
    ) -> bool:
        """Mark task as completed with results."""

        try:
            update_data = {
                "status": TaskStatus.COMPLETED,
                "processing_completed_at": datetime.utcnow(),
                "progress_percentage": 100,
                "result_data": result_data,
                "updated_at": datetime.utcnow()
            }

            # Add performance metrics to progress details
            if performance_metrics:
                result = await db.execute(select(Task.progress_details).where(Task.id == task_id))
                existing_details = result.scalar_one_or_none() or {}
                existing_details["performance_metrics"] = performance_metrics
                update_data["progress_details"] = existing_details

            await db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(update_data)
            )
            await db.commit()

            logger.info("Task completed successfully", task_id=task_id)
            return True

        except Exception as e:
            logger.error("Failed to complete task", task_id=task_id, error=str(e))
            await db.rollback()
            return False

    @staticmethod
    async def fail_task(
        db: AsyncSession,
        task_id: int,
        error: Exception,
        retry_eligible: bool = True
    ) -> bool:
        """Handle task failure with retry logic."""

        try:
            # Get current task to check retry count
            result = await db.execute(select(Task).where(Task.id == task_id))
            task = result.scalar_one_or_none()

            if not task:
                logger.error("Task not found for failure", task_id=task_id)
                return False

            # Determine if we should retry or fail permanently
            should_retry = (
                retry_eligible and
                task.retry_count < task.max_retries
            )

            update_data = {
                "last_error_at": datetime.utcnow(),
                "error_message": str(error),
                "error_details": {
                    "error_type": type(error).__name__,
                    "error_message": str(error),
                    "retry_count": task.retry_count + 1,
                    "timestamp": datetime.utcnow().isoformat()
                },
                "retry_count": task.retry_count + 1,
                "updated_at": datetime.utcnow()
            }

            if should_retry:
                # Reset to pending for retry
                update_data["status"] = TaskStatus.PENDING
                update_data["processing_started_at"] = None
                update_data["worker_id"] = None
                logger.info(
                    "Task marked for retry",
                    task_id=task_id,
                    retry_count=task.retry_count + 1,
                    error=str(error)
                )
            else:
                # Mark as permanently failed
                update_data["status"] = TaskStatus.FAILED
                update_data["processing_completed_at"] = datetime.utcnow()
                logger.error(
                    "Task failed permanently",
                    task_id=task_id,
                    retry_count=task.retry_count + 1,
                    error=str(error)
                )

            await db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(update_data)
            )
            await db.commit()

            return True

        except Exception as e:
            logger.error("Failed to handle task failure", task_id=task_id, error=str(e))
            await db.rollback()
            return False

    @staticmethod
    async def get_task_status(db: AsyncSession, task_id: int) -> Optional[Dict[str, Any]]:
        """Get detailed task status information."""

        try:
            result = await db.execute(
                select(Task).where(Task.id == task_id)
            )
            task = result.scalar_one_or_none()

            if not task:
                return None

            # Calculate duration if processing
            duration_seconds = None
            if task.processing_started_at:
                end_time = task.processing_completed_at or datetime.utcnow()
                duration_seconds = (end_time - task.processing_started_at).total_seconds()

            return {
                "task_id": task.id,
                "status": task.status,
                "progress_percentage": task.progress_percentage,
                "current_step": task.progress_details.get("current_step"),
                "estimated_completion_at": task.estimated_completion_at,
                "duration_seconds": duration_seconds,
                "retry_count": task.retry_count,
                "memory_usage_mb": task.memory_usage_mb,
                "worker_id": task.worker_id,
                "error_message": task.error_message,
                "last_updated": task.updated_at
            }

        except Exception as e:
            logger.error("Failed to get task status", task_id=task_id, error=str(e))
            return None

    @staticmethod
    async def get_active_task_count(db: AsyncSession) -> int:
        """Get count of currently active tasks."""

        try:
            result = await db.execute(
                select(Task)
                .where(Task.status.in_([TaskStatus.IN_PROGRESS, TaskStatus.PENDING]))
            )
            tasks = result.scalars().all()
            return len(tasks)

        except Exception as e:
            logger.error("Failed to get active task count", error=str(e))
            return 0

    @staticmethod
    async def cleanup_stale_tasks(db: AsyncSession, timeout_minutes: int = 30) -> int:
        """Cleanup tasks that have been processing too long."""

        try:
            cutoff_time = datetime.utcnow() - timedelta(minutes=timeout_minutes)

            # Find stale processing tasks
            result = await db.execute(
                select(Task)
                .where(
                    Task.status == TaskStatus.IN_PROGRESS,
                    Task.processing_started_at < cutoff_time
                )
            )
            stale_tasks = result.scalars().all()

            # Mark them as failed
            for task in stale_tasks:
                await TaskStatusService.fail_task(
                    db,
                    task.id,
                    Exception(f"Task timeout after {timeout_minutes} minutes"),
                    retry_eligible=True
                )

            logger.info("Cleaned up stale tasks", count=len(stale_tasks))
            return len(stale_tasks)

        except Exception as e:
            logger.error("Failed to cleanup stale tasks", error=str(e))
            return 0
```

---

## ðŸ› ï¸ **TASK 3: BROWSER CONTEXT MANAGEMENT**

### **File to Create:**
- `app/utils/browser_pool.py`

### **Complete Implementation:**

```python
# app/utils/browser_pool.py
import asyncio
from typing import Dict, Optional, List
from datetime import datetime, timedelta
import structlog
from playwright.async_api import async_playwright, BrowserContext, Browser, Page
import psutil
import uuid

from app.core.config import settings

logger = structlog.get_logger(__name__)

class BrowserPoolManager:
    """Manages a pool of browser contexts for efficient resource utilization."""

    def __init__(self):
        self.pool_size = getattr(settings, 'BROWSER_POOL_SIZE', 5)
        self.max_context_age_minutes = getattr(settings, 'BROWSER_CONTEXT_MAX_AGE_MINUTES', 30)
        self.max_memory_mb = getattr(settings, 'BROWSER_MAX_MEMORY_MB', 512)

        # Pool management
        self.available_contexts: asyncio.Queue = asyncio.Queue(maxsize=self.pool_size)
        self.active_contexts: Dict[str, Dict] = {}  # context_id -> context_info
        self.browser: Optional[Browser] = None
        self.playwright = None

        # Stats
        self.contexts_created = 0
        self.contexts_reused = 0
        self.contexts_cleanup = 0

    async def initialize(self):
        """Initialize the browser pool."""
        try:
            self.playwright = await async_playwright().start()

            # Launch browser with optimized settings
            self.browser = await self.playwright.chromium.launch(
                headless=getattr(settings, 'HEADLESS', True),
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-extensions',
                    '--disable-default-apps',
                    '--disable-background-timer-throttling',
                    '--disable-renderer-backgrounding',
                    '--disable-backgrounding-occluded-windows',
                    f'--memory-pressure-off',
                    f'--max_old_space_size={self.max_memory_mb}'
                ]
            )

            # Pre-warm the pool with contexts
            await self._populate_pool()

            logger.info("Browser pool initialized", pool_size=self.pool_size)

        except Exception as e:
            logger.error("Failed to initialize browser pool", error=str(e))
            raise

    async def acquire_context(self, task_id: int) -> BrowserContext:
        """Acquire a browser context for task execution."""

        context_id = str(uuid.uuid4())

        try:
            # Try to get an available context from pool
            try:
                context_info = await asyncio.wait_for(
                    self.available_contexts.get(),
                    timeout=10.0
                )
                context = context_info['context']

                # Check if context is still healthy
                if await self._is_context_healthy(context):
                    # Update context assignment
                    self.active_contexts[context_id] = {
                        'context': context,
                        'task_id': task_id,
                        'assigned_at': datetime.utcnow(),
                        'pages_created': 0,
                        'reused': True
                    }

                    self.contexts_reused += 1
                    logger.debug("Context reused from pool", task_id=task_id, context_id=context_id)
                    return context
                else:
                    # Context unhealthy, close it and create new one
                    await self._cleanup_context(context)

            except asyncio.TimeoutError:
                logger.warning("No available contexts in pool, creating new one", task_id=task_id)

            # Create new context
            context = await self._create_new_context()

            self.active_contexts[context_id] = {
                'context': context,
                'task_id': task_id,
                'assigned_at': datetime.utcnow(),
                'pages_created': 0,
                'reused': False
            }

            self.contexts_created += 1
            logger.debug("New context created", task_id=task_id, context_id=context_id)
            return context

        except Exception as e:
            logger.error("Failed to acquire browser context", task_id=task_id, error=str(e))
            raise

    async def release_context(self, task_id: int, context: BrowserContext):
        """Release browser context back to pool or cleanup."""

        try:
            # Find context in active contexts
            context_id = None
            context_info = None

            for cid, info in self.active_contexts.items():
                if info['context'] == context and info['task_id'] == task_id:
                    context_id = cid
                    context_info = info
                    break

            if not context_id or not context_info:
                logger.warning("Context not found in active contexts", task_id=task_id)
                await self._cleanup_context(context)
                return

            # Remove from active contexts
            del self.active_contexts[context_id]

            # Check if context should be returned to pool or cleaned up
            assigned_duration = datetime.utcnow() - context_info['assigned_at']

            if (assigned_duration.total_seconds() / 60 > self.max_context_age_minutes or
                not await self._is_context_healthy(context) or
                self.available_contexts.full()):

                # Cleanup old or unhealthy context
                await self._cleanup_context(context)
                self.contexts_cleanup += 1
                logger.debug("Context cleaned up", task_id=task_id, context_id=context_id)
            else:
                # Return to pool
                try:
                    # Clear any existing pages in context
                    for page in context.pages:
                        await page.close()

                    # Reset context state
                    await self._reset_context_state(context)

                    # Return to pool
                    await self.available_contexts.put({
                        'context': context,
                        'created_at': datetime.utcnow(),
                        'usage_count': context_info.get('usage_count', 0) + 1
                    })

                    logger.debug("Context returned to pool", task_id=task_id, context_id=context_id)

                except asyncio.QueueFull:
                    # Pool is full, cleanup instead
                    await self._cleanup_context(context)

        except Exception as e:
            logger.error("Failed to release browser context", task_id=task_id, error=str(e))
            await self._cleanup_context(context)

    async def _create_new_context(self) -> BrowserContext:
        """Create a new browser context with optimized settings."""

        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=self._get_random_user_agent(),
            ignore_https_errors=True,
            bypass_csp=True,
            java_script_enabled=True
        )

        # Enable request interception for performance
        await context.route("**/*", self._handle_route)

        return context

    async def _handle_route(self, route):
        """Handle browser requests for performance optimization."""

        # Block unnecessary resources for faster loading
        resource_type = route.request.resource_type

        if resource_type in ['image', 'media', 'font', 'stylesheet']:
            # Block non-essential resources unless specifically needed
            await route.abort()
        else:
            await route.continue_()

    async def _is_context_healthy(self, context: BrowserContext) -> bool:
        """Check if browser context is healthy and functional."""

        try:
            # Try to create and close a test page
            page = await context.new_page()
            await page.goto("data:text/html,<html><body>test</body></html>")
            await page.close()
            return True

        except Exception as e:
            logger.warning("Context health check failed", error=str(e))
            return False

    async def _reset_context_state(self, context: BrowserContext):
        """Reset context state for reuse."""

        try:
            # Clear cookies and storage
            await context.clear_cookies()
            await context.clear_permissions()

            # Close any remaining pages
            for page in context.pages:
                await page.close()

        except Exception as e:
            logger.warning("Failed to reset context state", error=str(e))

    async def _cleanup_context(self, context: BrowserContext):
        """Cleanup a browser context."""

        try:
            await context.close()
        except Exception as e:
            logger.warning("Failed to cleanup context", error=str(e))

    async def _populate_pool(self):
        """Pre-populate the context pool."""

        for i in range(min(2, self.pool_size)):  # Start with 2 pre-warmed contexts
            try:
                context = await self._create_new_context()
                await self.available_contexts.put({
                    'context': context,
                    'created_at': datetime.utcnow(),
                    'usage_count': 0
                })
            except Exception as e:
                logger.warning("Failed to pre-populate context pool", error=str(e))

    def _get_random_user_agent(self) -> str:
        """Get a random user agent for anti-detection."""

        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]

        import random
        return random.choice(user_agents)

    async def get_pool_status(self) -> Dict:
        """Get current pool status for monitoring."""

        return {
            'available_contexts': self.available_contexts.qsize(),
            'active_contexts': len(self.active_contexts),
            'total_contexts_created': self.contexts_created,
            'total_contexts_reused': self.contexts_reused,
            'total_contexts_cleanup': self.contexts_cleanup,
            'pool_size': self.pool_size
        }

    async def cleanup_all(self):
        """Cleanup all contexts and close browser."""

        try:
            # Cleanup active contexts
            for context_info in self.active_contexts.values():
                await self._cleanup_context(context_info['context'])

            # Cleanup available contexts
            while not self.available_contexts.empty():
                context_info = await self.available_contexts.get()
                await self._cleanup_context(context_info['context'])

            # Close browser
            if self.browser:
                await self.browser.close()

            # Stop playwright
            if self.playwright:
                await self.playwright.stop()

            logger.info("Browser pool cleaned up successfully")

        except Exception as e:
            logger.error("Failed to cleanup browser pool", error=str(e))

# Global browser pool instance
browser_pool = BrowserPoolManager()
```

---

## ðŸ› ï¸ **TASK 4: BASIC WEBPARSER SERVICE**

### **File to Create:**
- `app/services/web_parser.py`

### **Complete Implementation (Basic Version):**

```python
# app/services/web_parser.py
from typing import Optional, Dict, Any, List
from datetime import datetime
import asyncio
import structlog
import hashlib
import json
from urllib.parse import urljoin, urlparse

from playwright.async_api import BrowserContext, Page
from sqlalchemy.ext.asyncio import AsyncSession

from app.utils.browser_pool import browser_pool
from app.services.task_status_service import TaskStatusService
from app.models.task import Task, TaskStatus
from app.models.web_page import WebPage
from app.models.interactive_element import InteractiveElement, ElementType, InteractionType
from app.schemas.web_page import WebPageParseRequest, WebPageParseResponse, WebPageCreate
from app.core.config import settings

logger = structlog.get_logger(__name__)

class WebParserService:
    """Service for parsing webpages and extracting semantic information."""

    def __init__(self):
        self.timeout_seconds = getattr(settings, 'BROWSER_TIMEOUT', 30)
        self.max_elements_per_page = getattr(settings, 'MAX_ELEMENTS_PER_PAGE', 100)

    async def parse_webpage_async(
        self,
        db: AsyncSession,
        task_id: int,
        url: str,
        options: WebPageParseRequest
    ) -> WebPageParseResponse:
        """Main async parsing method for background execution."""

        start_time = datetime.utcnow()
        context = None

        try:
            # Update task status - starting
            await TaskStatusService.mark_task_processing(
                db, task_id, f"webparser-{task_id}"
            )

            await TaskStatusService.update_task_progress(
                db, task_id,
                progress_percentage=10,
                current_step="acquiring_browser_context"
            )

            # Acquire browser context
            context = await browser_pool.acquire_context(task_id)

            await TaskStatusService.update_task_progress(
                db, task_id,
                progress_percentage=20,
                current_step="navigating_to_page"
            )

            # Perform the actual parsing
            result = await self._perform_parsing(db, context, url, options, task_id)

            # Calculate processing time
            processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000

            # Update task completion
            await TaskStatusService.complete_task(
                db, task_id,
                result.dict(),
                {"processing_time_ms": processing_time}
            )

            logger.info(
                "Webpage parsing completed successfully",
                task_id=task_id,
                url=url,
                processing_time_ms=processing_time
            )

            return result

        except Exception as e:
            logger.error("Webpage parsing failed", task_id=task_id, url=url, error=str(e))
            await TaskStatusService.fail_task(db, task_id, e)
            raise

        finally:
            if context:
                await browser_pool.release_context(task_id, context)

    async def _perform_parsing(
        self,
        db: AsyncSession,
        context: BrowserContext,
        url: str,
        options: WebPageParseRequest,
        task_id: int
    ) -> WebPageParseResponse:
        """Perform the actual webpage parsing with progress updates."""

        page = await context.new_page()

        try:
            # Step 1: Navigate to page
            await TaskStatusService.update_task_progress(
                db, task_id,
                progress_percentage=30,
                current_step="loading_webpage"
            )

            await page.goto(
                str(url),
                wait_until="networkidle",
                timeout=self.timeout_seconds * 1000
            )

            # Wait for additional dynamic content if specified
            if options.wait_for_load > 0:
                await asyncio.sleep(options.wait_for_load)

            # Step 2: Extract page metadata
            await TaskStatusService.update_task_progress(
                db, task_id,
                progress_percentage=45,
                current_step="extracting_page_metadata"
            )

            page_metadata = await self._extract_page_metadata(page)

            # Step 3: Extract interactive elements
            await TaskStatusService.update_task_progress(
                db, task_id,
                progress_percentage=60,
                current_step="identifying_interactive_elements"
            )

            interactive_elements = await self._extract_interactive_elements(page)

            # Step 4: Extract content blocks
            await TaskStatusService.update_task_progress(
                db, task_id,
                progress_percentage=75,
                current_step="extracting_content_blocks"
            )

            content_blocks = await self._extract_content_blocks(page)

            # Step 5: Take screenshot if requested
            screenshot_paths = []
            if options.include_screenshot:
                await TaskStatusService.update_task_progress(
                    db, task_id,
                    progress_percentage=85,
                    current_step="capturing_screenshot"
                )

                screenshot_path = await self._capture_screenshot(page, task_id)
                if screenshot_path:
                    screenshot_paths.append(screenshot_path)

            # Step 6: Store webpage data in database
            await TaskStatusService.update_task_progress(
                db, task_id,
                progress_percentage=95,
                current_step="storing_webpage_data"
            )

            web_page = await self._store_webpage_data(
                db, url, page_metadata, interactive_elements, content_blocks
            )

            return WebPageParseResponse(
                web_page=web_page,
                processing_time_ms=0,  # Will be set by caller
                cache_hit=False,
                screenshots=screenshot_paths,
                warnings=[],
                errors=[]
            )

        finally:
            await page.close()

    async def _extract_page_metadata(self, page: Page) -> Dict[str, Any]:
        """Extract basic page metadata."""

        try:
            # Get page title
            title = await page.title()

            # Get page URL (may be different from requested due to redirects)
            current_url = page.url

            # Extract meta tags
            meta_description = await page.get_attribute('meta[name="description"]', 'content') or ""
            meta_keywords = await page.get_attribute('meta[name="keywords"]', 'content') or ""

            # Get page language
            language = await page.get_attribute('html', 'lang') or "en"

            # Check for JavaScript
            has_javascript = await page.evaluate('() => window.navigator.javaEnabled()')

            # Basic structure analysis
            form_count = len(await page.query_selector_all('form'))
            link_count = len(await page.query_selector_all('a[href]'))
            image_count = len(await page.query_selector_all('img'))

            return {
                'title': title,
                'current_url': current_url,
                'meta_description': meta_description,
                'meta_keywords': meta_keywords,
                'language': language,
                'has_javascript': has_javascript,
                'form_count': form_count,
                'link_count': link_count,
                'image_count': image_count,
                'parsed_at': datetime.utcnow().isoformat()
            }

        except Exception as e:
            logger.warning("Failed to extract page metadata", error=str(e))
            return {
                'title': '',
                'current_url': page.url,
                'has_javascript': False,
                'form_count': 0,
                'link_count': 0,
                'image_count': 0,
                'parsed_at': datetime.utcnow().isoformat()
            }

    async def _extract_interactive_elements(self, page: Page) -> List[Dict[str, Any]]:
        """Extract interactive elements from the page."""

        elements = []

        try:
            # Define selectors for different element types
            selectors = {
                'button': 'button, input[type="button"], input[type="submit"], [role="button"]',
                'input': 'input:not([type="button"]):not([type="submit"])',
                'select': 'select',
                'textarea': 'textarea',
                'link': 'a[href]',
                'checkbox': 'input[type="checkbox"]',
                'radio': 'input[type="radio"]',
                'file_upload': 'input[type="file"]'
            }

            for element_type, selector in selectors.items():
                # Find elements of this type
                found_elements = await page.query_selector_all(selector)

                for i, element in enumerate(found_elements[:self.max_elements_per_page]):
                    try:
                        # Extract element properties
                        element_data = await self._extract_element_properties(
                            element, element_type, i, page
                        )

                        if element_data:
                            elements.append(element_data)

                    except Exception as e:
                        logger.warning("Failed to extract element", element_type=element_type, error=str(e))

            logger.debug("Extracted interactive elements", count=len(elements))
            return elements[:self.max_elements_per_page]  # Limit total elements

        except Exception as e:
            logger.error("Failed to extract interactive elements", error=str(e))
            return []

    async def _extract_element_properties(
        self,
        element,
        element_type: str,
        index: int,
        page: Page
    ) -> Optional[Dict[str, Any]]:
        """Extract properties from a single element."""

        try:
            # Basic properties
            tag_name = await element.evaluate('el => el.tagName.toLowerCase()')
            element_id = await element.get_attribute('id') or ""
            element_class = await element.get_attribute('class') or ""

            # Text content
            text_content = (await element.text_content() or "").strip()
            placeholder = await element.get_attribute('placeholder') or ""
            value = await element.get_attribute('value') or ""

            # ARIA properties
            aria_label = await element.get_attribute('aria-label') or ""
            title = await element.get_attribute('title') or ""

            # Position and visibility
            is_visible = await element.is_visible()
            is_enabled = await element.is_enabled()

            # Bounding box for coordinates
            bbox = await element.bounding_box()
            x_coordinate = int(bbox['x']) if bbox else 0
            y_coordinate = int(bbox['y']) if bbox else 0
            width = int(bbox['width']) if bbox else 0
            height = int(bbox['height']) if bbox else 0

            # Generate XPath
            xpath = await element.evaluate('''
                el => {
                    const getXPath = (element) => {
                        if (element.id) return `//*[@id="${element.id}"]`;
                        if (element === document.body) return '/html/body';

                        let ix = 0;
                        const siblings = element.parentNode?.childNodes || [];
                        for (let i = 0; i < siblings.length; i++) {
                            const sibling = siblings[i];
                            if (sibling === element) {
                                return getXPath(element.parentNode) + '/' + element.tagName.toLowerCase() + '[' + (ix + 1) + ']';
                            }
                            if (sibling.nodeType === 1 && sibling.tagName === element.tagName) {
                                ix++;
                            }
                        }
                        return '';
                    };
                    return getXPath(el);
                }
            ''')

            # Generate CSS selector
            css_selector = await self._generate_css_selector(
                element, element_id, element_class, tag_name
            )

            # Determine supported interactions
            supported_interactions = await self._determine_interactions(element, element_type)

            return {
                'element_id': element_id,
                'element_class': element_class,
                'tag_name': tag_name,
                'element_type': element_type,
                'text_content': text_content[:500],  # Limit length
                'placeholder': placeholder,
                'value': value,
                'aria_label': aria_label,
                'title': title,
                'xpath': xpath,
                'css_selector': css_selector,
                'element_index': index,
                'x_coordinate': x_coordinate,
                'y_coordinate': y_coordinate,
                'width': width,
                'height': height,
                'is_visible': is_visible,
                'is_enabled': is_enabled,
                'supported_interactions': supported_interactions,
                'interaction_confidence': 0.8,  # Default confidence
                'semantic_role': self._determine_semantic_role(element_type, text_content),
                'discovered_at': datetime.utcnow().isoformat()
            }

        except Exception as e:
            logger.warning("Failed to extract element properties", error=str(e))
            return None

    async def _generate_css_selector(
        self,
        element,
        element_id: str,
        element_class: str,
        tag_name: str
    ) -> str:
        """Generate a CSS selector for the element."""

        try:
            # Prefer ID selector
            if element_id:
                return f"#{element_id}"

            # Try class selector
            if element_class:
                classes = element_class.split()
                if classes:
                    return f"{tag_name}.{'.'.join(classes[:2])}"  # Limit to 2 classes

            # Fallback to tag name
            return tag_name

        except Exception:
            return tag_name

    async def _determine_interactions(self, element, element_type: str) -> List[str]:
        """Determine what interactions are possible with this element."""

        interactions = []

        try:
            if element_type in ['button', 'link']:
                interactions.append('click')
            elif element_type in ['input', 'textarea']:
                interactions.extend(['click', 'type'])
            elif element_type == 'select':
                interactions.extend(['click', 'select'])
            elif element_type in ['checkbox', 'radio']:
                interactions.append('click')
            elif element_type == 'file_upload':
                interactions.extend(['click', 'upload'])

            # Check if element is clickable
            if await element.is_visible() and await element.is_enabled():
                if 'click' not in interactions:
                    # Check if element has click handlers
                    has_onclick = await element.get_attribute('onclick')
                    if has_onclick or element_type in ['div', 'span']:
                        interactions.append('click')

            return interactions

        except Exception:
            return ['click'] if element_type == 'button' else []

    def _determine_semantic_role(self, element_type: str, text_content: str) -> str:
        """Determine semantic role of element based on type and content."""

        text_lower = text_content.lower()

        if element_type == 'button':
            if any(word in text_lower for word in ['submit', 'send', 'save']):
                return 'submit'
            elif any(word in text_lower for word in ['cancel', 'close', 'back']):
                return 'cancel'
            else:
                return 'action'
        elif element_type == 'link':
            return 'navigation'
        elif element_type in ['input', 'textarea']:
            return 'form_input'
        else:
            return 'content'

    async def _extract_content_blocks(self, page: Page) -> List[Dict[str, Any]]:
        """Extract content blocks from the page."""

        # Basic implementation - extract text blocks
        content_blocks = []

        try:
            # Extract headings
            headings = await page.query_selector_all('h1, h2, h3, h4, h5, h6')
            for i, heading in enumerate(headings[:20]):  # Limit headings
                text = await heading.text_content()
                if text and text.strip():
                    content_blocks.append({
                        'block_type': 'heading',
                        'text_content': text.strip()[:200],
                        'dom_path': f'heading_{i}',
                        'semantic_importance': 0.8
                    })

            # Extract paragraphs
            paragraphs = await page.query_selector_all('p')
            for i, para in enumerate(paragraphs[:10]):  # Limit paragraphs
                text = await para.text_content()
                if text and len(text.strip()) > 20:  # Only meaningful paragraphs
                    content_blocks.append({
                        'block_type': 'text',
                        'text_content': text.strip()[:500],
                        'dom_path': f'paragraph_{i}',
                        'semantic_importance': 0.5
                    })

            return content_blocks

        except Exception as e:
            logger.warning("Failed to extract content blocks", error=str(e))
            return []

    async def _capture_screenshot(self, page: Page, task_id: int) -> Optional[str]:
        """Capture screenshot of the page."""

        try:
            # Create screenshots directory if it doesn't exist
            import os
            screenshot_dir = "screenshots"
            os.makedirs(screenshot_dir, exist_ok=True)

            # Generate filename
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filename = f"task_{task_id}_{timestamp}.png"
            filepath = os.path.join(screenshot_dir, filename)

            # Take screenshot
            await page.screenshot(path=filepath, full_page=True)

            logger.debug("Screenshot captured", task_id=task_id, path=filepath)
            return filepath

        except Exception as e:
            logger.warning("Failed to capture screenshot", task_id=task_id, error=str(e))
            return None

    async def _store_webpage_data(
        self,
        db: AsyncSession,
        url: str,
        metadata: Dict[str, Any],
        interactive_elements: List[Dict[str, Any]],
        content_blocks: List[Dict[str, Any]]
    ) -> WebPage:
        """Store parsed webpage data in the database."""

        try:
            # Create content hash
            content_for_hash = json.dumps({
                'url': url,
                'title': metadata.get('title', ''),
                'elements_count': len(interactive_elements)
            }, sort_keys=True)
            content_hash = hashlib.sha256(content_for_hash.encode()).hexdigest()

            # Extract domain
            domain = urlparse(url).netloc

            # Create webpage record
            web_page_data = {
                'url': url,
                'canonical_url': metadata.get('current_url', url),
                'title': metadata.get('title', ''),
                'domain': domain,
                'content_hash': content_hash,
                'language': metadata.get('language', 'en'),
                'interactive_elements_count': len(interactive_elements),
                'form_count': metadata.get('form_count', 0),
                'link_count': metadata.get('link_count', 0),
                'image_count': metadata.get('image_count', 0),
                'has_javascript': metadata.get('has_javascript', False),
                'semantic_data': {
                    'metadata': metadata,
                    'parsing_summary': {
                        'interactive_elements': len(interactive_elements),
                        'content_blocks': len(content_blocks),
                        'parsed_at': datetime.utcnow().isoformat()
                    }
                },
                'success': True
            }

            # Create WebPage object (this should integrate with actual DB storage)
            # For now, return a mock WebPage object
            from app.schemas.web_page import WebPage as WebPageSchema

            return WebPageSchema(**web_page_data)

        except Exception as e:
            logger.error("Failed to store webpage data", error=str(e))
            raise

# Initialize service instance
web_parser_service = WebParserService()
```

---

## ðŸ› ï¸ **TASK 5: IMPLEMENTATION OF PARSE ENDPOINT**

### **File to Modify:**
- `app/api/v1/endpoints/web_pages.py`

### **Replace the parse_webpage function:**

```python
# Replace the existing parse_webpage function in app/api/v1/endpoints/web_pages.py
from fastapi import BackgroundTasks
from app.services.web_parser import web_parser_service

@router.post("/parse", response_model=dict)  # Changed to dict for immediate response
async def parse_webpage(
    parse_request: WebPageParseRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Parse a webpage and extract semantic information.

    Returns immediately with task_id for tracking progress.
    Use GET /tasks/{task_id} to check status and results.
    """
    try:
        # Create task record for tracking
        from app.services.task_service import task_service

        task_data = {
            'title': f"Parse webpage: {parse_request.url}",
            'description': f"Parsing {parse_request.url} for interactive elements and content",
            'goal': f"extract_webpage_data:{parse_request.url}",
            'target_url': str(parse_request.url),
            'user_id': current_user.id,
            'priority': 'medium'
        }

        # Create task in database
        task = await task_service.create_task(db, current_user.id, task_data)

        # Queue background processing
        background_tasks.add_task(
            _process_webpage_parsing,
            task_id=task.id,
            url=str(parse_request.url),
            options=parse_request,
            db_session=db  # Note: This won't work directly, need to handle differently
        )

        logger.info(
            "Webpage parsing queued",
            task_id=task.id,
            url=parse_request.url,
            user_id=current_user.id
        )

        # Return immediately with task information
        return {
            "task_id": task.id,
            "status": "queued",
            "message": "Webpage parsing started",
            "estimated_duration_seconds": 30,
            "check_status_url": f"/api/v1/tasks/{task.id}",
            "url": str(parse_request.url)
        }

    except Exception as e:
        logger.error("Failed to queue webpage parsing", error=str(e), url=parse_request.url)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to start webpage parsing: {str(e)}"
        )

async def _process_webpage_parsing(
    task_id: int,
    url: str,
    options: WebPageParseRequest
):
    """Background function to process webpage parsing."""

    # Get new database session for background task
    from app.db.session import get_async_session

    async for db in get_async_session():
        try:
            # Process the webpage
            result = await web_parser_service.parse_webpage_async(
                db, task_id, url, options
            )

            logger.info("Background webpage parsing completed", task_id=task_id, url=url)
            break  # Exit the async generator

        except Exception as e:
            logger.error("Background webpage parsing failed", task_id=task_id, url=url, error=str(e))

            # Update task status to failed
            from app.services.task_status_service import TaskStatusService
            await TaskStatusService.fail_task(db, task_id, e)
            break
```

---

## ðŸ› ï¸ **TASK 6: TASK SERVICE FOR CRUD OPERATIONS**

### **File to Create:**
- `app/services/task_service.py`

### **Complete Implementation:**

```python
# app/services/task_service.py
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update, delete, and_, or_
from sqlalchemy.orm import selectinload
from typing import Optional, List, Dict, Any, Tuple
from datetime import datetime
import structlog

from app.models.task import Task, TaskStatus, TaskPriority
from app.models.user import User
from app.schemas.task import TaskCreate, TaskUpdate, TaskFilters, TaskStats
from app.core.config import settings

logger = structlog.get_logger(__name__)

class TaskService:
    """Service for task CRUD operations and business logic."""

    @staticmethod
    async def create_task(
        db: AsyncSession,
        user_id: int,
        task_data: Dict[str, Any]
    ) -> Task:
        """Create a new task."""

        try:
            # Create task record
            task = Task(
                user_id=user_id,
                title=task_data.get('title', ''),
                description=task_data.get('description', ''),
                goal=task_data.get('goal', ''),
                target_url=task_data.get('target_url'),
                priority=task_data.get('priority', TaskPriority.MEDIUM),
                status=TaskStatus.PENDING,
                max_retries=task_data.get('max_retries', 3),
                timeout_seconds=task_data.get('timeout_seconds', 300),
                require_confirmation=task_data.get('require_confirmation', False),
                allow_sensitive_actions=task_data.get('allow_sensitive_actions', False)
            )

            db.add(task)
            await db.commit()
            await db.refresh(task)

            logger.info("Task created successfully", task_id=task.id, user_id=user_id)
            return task

        except Exception as e:
            logger.error("Failed to create task", user_id=user_id, error=str(e))
            await db.rollback()
            raise

    @staticmethod
    async def get_task_by_id(
        db: AsyncSession,
        task_id: int,
        user_id: int
    ) -> Optional[Task]:
        """Get task by ID, ensuring user owns the task."""

        try:
            result = await db.execute(
                select(Task)
                .where(and_(Task.id == task_id, Task.user_id == user_id))
                .options(selectinload(Task.execution_plan))
            )

            return result.scalar_one_or_none()

        except Exception as e:
            logger.error("Failed to get task", task_id=task_id, user_id=user_id, error=str(e))
            return None

    @staticmethod
    async def get_user_tasks(
        db: AsyncSession,
        user_id: int,
        filters: Optional[TaskFilters] = None,
        page: int = 1,
        page_size: int = 20
    ) -> Tuple[List[Task], int]:
        """Get user's tasks with pagination and filtering."""

        try:
            # Build base query
            query = select(Task).where(Task.user_id == user_id)

            # Apply filters
            if filters:
                if filters.status:
                    query = query.where(Task.status == filters.status)

                if filters.priority:
                    query = query.where(Task.priority == filters.priority)

                if filters.created_after:
                    query = query.where(Task.created_at >= filters.created_after)

                if filters.created_before:
                    query = query.where(Task.created_at <= filters.created_before)

                if filters.domain:
                    query = query.where(Task.target_url.ilike(f"%{filters.domain}%"))

                if filters.search_query:
                    search_pattern = f"%{filters.search_query}%"
                    query = query.where(
                        or_(
                            Task.title.ilike(search_pattern),
                            Task.description.ilike(search_pattern),
                            Task.goal.ilike(search_pattern)
                        )
                    )

            # Add ordering
            query = query.order_by(Task.created_at.desc())

            # Count total results
            count_query = select(Task.id).where(Task.user_id == user_id)
            if filters:
                # Apply same filters to count query
                # ... (repeat filter logic for count)
                pass

            total_result = await db.execute(count_query)
            total_count = len(total_result.scalars().all())

            # Apply pagination
            offset = (page - 1) * page_size
            query = query.offset(offset).limit(page_size)

            # Execute query
            result = await db.execute(query)
            tasks = result.scalars().all()

            return list(tasks), total_count

        except Exception as e:
            logger.error("Failed to get user tasks", user_id=user_id, error=str(e))
            return [], 0

    @staticmethod
    async def update_task(
        db: AsyncSession,
        task_id: int,
        user_id: int,
        task_data: TaskUpdate
    ) -> Optional[Task]:
        """Update an existing task."""

        try:
            # Check task exists and user owns it
            task = await TaskService.get_task_by_id(db, task_id, user_id)
            if not task:
                return None

            # Check if task can be updated (not executing)
            if task.status == TaskStatus.IN_PROGRESS:
                raise ValueError("Cannot update task while it's executing")

            # Build update data
            update_data = {"updated_at": datetime.utcnow()}

            if task_data.title is not None:
                update_data["title"] = task_data.title
            if task_data.description is not None:
                update_data["description"] = task_data.description
            if task_data.priority is not None:
                update_data["priority"] = task_data.priority
            if task_data.status is not None:
                update_data["status"] = task_data.status
            if task_data.require_confirmation is not None:
                update_data["require_confirmation"] = task_data.require_confirmation
            if task_data.allow_sensitive_actions is not None:
                update_data["allow_sensitive_actions"] = task_data.allow_sensitive_actions

            # Execute update
            await db.execute(
                update(Task)
                .where(and_(Task.id == task_id, Task.user_id == user_id))
                .values(update_data)
            )
            await db.commit()

            # Return updated task
            return await TaskService.get_task_by_id(db, task_id, user_id)

        except Exception as e:
            logger.error("Failed to update task", task_id=task_id, user_id=user_id, error=str(e))
            await db.rollback()
            raise

    @staticmethod
    async def delete_task(
        db: AsyncSession,
        task_id: int,
        user_id: int
    ) -> bool:
        """Delete a task."""

        try:
            # Check task exists and user owns it
            task = await TaskService.get_task_by_id(db, task_id, user_id)
            if not task:
                return False

            # Check if task can be deleted (not executing)
            if task.status == TaskStatus.IN_PROGRESS:
                raise ValueError("Cannot delete task while it's executing")

            # Delete task
            await db.execute(
                delete(Task)
                .where(and_(Task.id == task_id, Task.user_id == user_id))
            )
            await db.commit()

            logger.info("Task deleted successfully", task_id=task_id, user_id=user_id)
            return True

        except Exception as e:
            logger.error("Failed to delete task", task_id=task_id, user_id=user_id, error=str(e))
            await db.rollback()
            raise

    @staticmethod
    async def get_task_stats(
        db: AsyncSession,
        user_id: int
    ) -> TaskStats:
        """Get user's task execution statistics."""

        try:
            # Get all user tasks
            result = await db.execute(
                select(Task).where(Task.user_id == user_id)
            )
            tasks = result.scalars().all()

            # Calculate statistics
            total_tasks = len(tasks)
            completed_tasks = len([t for t in tasks if t.status == TaskStatus.COMPLETED])
            failed_tasks = len([t for t in tasks if t.status == TaskStatus.FAILED])
            pending_tasks = len([t for t in tasks if t.status == TaskStatus.PENDING])

            success_rate = completed_tasks / total_tasks if total_tasks > 0 else 0.0

            # Calculate average duration for completed tasks
            completed_with_duration = [
                t for t in tasks
                if t.status == TaskStatus.COMPLETED and t.actual_duration_seconds
            ]

            average_duration = None
            if completed_with_duration:
                total_duration = sum(t.actual_duration_seconds for t in completed_with_duration)
                average_duration = total_duration / len(completed_with_duration)

            # Get most common domains
            domain_counts = {}
            for task in tasks:
                if task.target_url:
                    from urllib.parse import urlparse
                    try:
                        domain = urlparse(task.target_url).netloc
                        domain_counts[domain] = domain_counts.get(domain, 0) + 1
                    except:
                        pass

            most_common_domains = [
                {"domain": domain, "count": count}
                for domain, count in sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            ]

            # Recent activity (last 10 tasks)
            recent_tasks = sorted(tasks, key=lambda t: t.created_at, reverse=True)[:10]
            recent_activity = [
                {
                    "task_id": task.id,
                    "title": task.title,
                    "status": task.status,
                    "created_at": task.created_at.isoformat(),
                    "completed_at": task.completed_at.isoformat() if task.completed_at else None
                }
                for task in recent_tasks
            ]

            return TaskStats(
                total_tasks=total_tasks,
                completed_tasks=completed_tasks,
                failed_tasks=failed_tasks,
                pending_tasks=pending_tasks,
                success_rate=success_rate,
                average_duration_seconds=average_duration,
                most_common_domains=most_common_domains,
                recent_activity=recent_activity
            )

        except Exception as e:
            logger.error("Failed to get task stats", user_id=user_id, error=str(e))
            # Return empty stats on error
            return TaskStats(
                total_tasks=0,
                completed_tasks=0,
                failed_tasks=0,
                pending_tasks=0,
                success_rate=0.0,
                average_duration_seconds=None,
                most_common_domains=[],
                recent_activity=[]
            )

# Service instance
task_service = TaskService()
```

---

## ðŸ› ï¸ **TASK 7: RUN MIGRATION AND START BROWSER POOL**

### **Application Startup Enhancement:**

Add to `app/main.py` startup event:

```python
# Add to the startup_event function in app/main.py
@app.on_event("startup")
async def startup_event():
    """Application startup event handler."""
    logger.info(
        "WebAgent starting up",
        environment=settings.ENVIRONMENT,
        debug=settings.DEBUG,
        version=settings.APP_VERSION,
    )

    # Initialize database
    try:
        from app.db.session import get_async_session, check_database_connection
        from app.db.init_db import init_db

        # Check database connection
        if await check_database_connection():
            logger.info("Database connection successful")

            # Initialize database with required data
            async for db in get_async_session():
                await init_db(db)
                break
        else:
            logger.error("Database connection failed")

    except Exception as e:
        logger.error("Database initialization failed", error=str(e))

    # Initialize browser pool
    try:
        from app.utils.browser_pool import browser_pool
        await browser_pool.initialize()
        logger.info("Browser pool initialized successfully")
    except Exception as e:
        logger.error("Browser pool initialization failed", error=str(e))
```

### **Shutdown Enhancement:**

```python
# Add to the shutdown_event function in app/main.py
@app.on_event("shutdown")
async def shutdown_event():
    """Application shutdown event handler."""
    logger.info("WebAgent shutting down")

    # Cleanup browser pool
    try:
        from app.utils.browser_pool import browser_pool
        await browser_pool.cleanup_all()
        logger.info("Browser pool cleaned up")
    except Exception as e:
        logger.error("Error cleaning up browser pool", error=str(e))

    # Close database connections
    try:
        from app.db.session import close_async_engine
        await close_async_engine()
        logger.info("Database connections closed")
    except Exception as e:
        logger.error("Error closing database connections", error=str(e))
```

---

## ðŸ§ª **TESTING PLAN**

### **Manual Testing Steps:**

1. **Run Migration:**
   ```bash
   poetry run alembic upgrade head
   ```

2. **Start Application:**
   ```bash
   poetry run uvicorn app.main:app --reload
   ```

3. **Test Authentication (Already Working):**
   ```bash
   # Register user
   curl -X POST "http://localhost:8000/api/v1/auth/register" \
        -H "Content-Type: application/json" \
        -d '{"email":"test@example.com","username":"testuser","password":"Test123!","confirm_password":"Test123!","accept_terms":true}'

   # Login
   curl -X POST "http://localhost:8000/api/v1/auth/login" \
        -H "Content-Type: application/x-www-form-urlencoded" \
        -d "username=testuser&password=Test123!"
   ```

4. **Test Webpage Parsing:**
   ```bash
   # Parse a webpage (use token from login)
   curl -X POST "http://localhost:8000/api/v1/web-pages/parse" \
        -H "Authorization: Bearer YOUR_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"url":"https://example.com","include_screenshot":true,"wait_for_load":2}'
   ```

5. **Check Task Status:**
   ```bash
   # Check task status (use task_id from parse response)
   curl -X GET "http://localhost:8000/api/v1/tasks/TASK_ID" \
        -H "Authorization: Bearer YOUR_TOKEN"
   ```

---

## ðŸ“Š **SUCCESS CRITERIA**

### **Phase 2B Completion Checklist:**

- [ ] Database migration runs successfully
- [ ] Browser pool initializes without errors
- [ ] Background task processing works
- [ ] Webpage parsing returns task_id immediately
- [ ] Task status updates in real-time
- [ ] Browser contexts are properly managed
- [ ] Screenshots are captured and stored
- [ ] Interactive elements are extracted
- [ ] All endpoints return proper HTTP status codes
- [ ] Error handling works correctly

### **Performance Targets:**

- **API Response Time:** < 200ms for immediate task creation
- **Background Processing:** 30-60 seconds for typical webpage
- **Browser Context:** < 3 seconds to acquire from pool
- **Memory Usage:** < 512MB per browser context
- **Concurrent Tasks:** Support 5 simultaneous parsing operations

---

This implementation specification provides a complete roadmap for building the background task processing system. The architecture is designed for reliability, scalability, and excellent developer experience.

**Next Steps:** Implement these tasks in order, test each component, and then we'll move to Phase 2C with Task Planning and AI integration!
